<!DOCTYPE html>
<html>
<head>
  <style>
    /* Add some styles for the button */
    #record {
      width: 100px;
      height: 100px;
      border-radius: 50%;
      background-color: red;
      color: white;
      font-size: 24px;
    }

    #waves {
      width: 500px;
      height: 200px;
      border: 1px solid black;
      background-color: lightgray;
      padding: 10px;
    }
  </style>
</head>
<body>
  <button id="record">Record</button>
  <div id="waves"></div>
  
</body>
</html>
<script>
// Get a reference to the SpeechRecognition interface
window.SpeechRecognition = window.webkitSpeechRecognition || window.SpeechRecognition;

// Check if the browser supports the Web Speech API
if ('SpeechRecognition' in window) {
  // Create a new instance of SpeechRecognition
  const recognition = new SpeechRecognition();

  // Set some options for the recognition
  recognition.lang = 'en-US'; // Set the language to English
  recognition.interimResults = false; // Don't return intermediate results
  recognition.maxAlternatives = 1; // Return only one result

  // Get a reference to the button and the div elements
  const record = document.getElementById('record');
  const waves = document.getElementById('waves');

  // Create a canvas element to draw the frequency values
  const canvas = document.createElement('div');
  canvas.style.width = '100%';
  canvas.style.height = '100%';
  canvas.style.display = 'flex';
  canvas.style.alignItems = 'center';
  canvas.style.justifyContent = 'center';
  waves.appendChild(canvas);

  // Create an audio context to process the audio data
  const audioCtx = new AudioContext();
  const analyser = audioCtx.createAnalyser(); // Create an analyser node
  analyser.fftSize = 256; // Set the FFT size
  const bufferLength = analyser.frequencyBinCount; // Get the buffer length
  const dataArray = new Uint8Array(bufferLength); // Create an array to store the data

  // Define a function to draw the frequency values
  function draw() {
    // Request the next animation frame
    requestAnimationFrame(draw);

    // Get the frequency data from the analyser
    analyser.getByteFrequencyData(dataArray);

    // Display the frequency values in the #waves div
    const frequencyValues = Array.from(dataArray);
    canvas.textContent = frequencyValues.join(', ');
  }

  // Add an event listener to the button element
  record.addEventListener('click', function () {
    // Check if the recognition is already running
    if (!recognition.isRecording) {
      // Start the recognition
      recognition.start();
      // Change the button color and text
      record.style.backgroundColor = 'green';
      record.textContent = 'Stop';
    } else {
      // Stop the recognition
      recognition.stop();
      // Change the button color and text
      record.style.backgroundColor = 'red';
      record.textContent = 'Record';
    }
  });

  // Add an event listener to the recognition start event
  recognition.addEventListener('start', function () {
    // Create a media stream source from the microphone
    navigator.mediaDevices.getUserMedia({ audio: true }).then(function (stream) {
      // Connect the source to the analyser node
      const source = audioCtx.createMediaStreamSource(stream);
      source.connect(analyser);
      // Start drawing the frequency values
      draw();
    });
  });

  // Add an event listener to the recognition end event
  recognition.addEventListener('end', function () {
    // Stop drawing the frequency values
    cancelAnimationFrame(draw);
  });

  // Add an event listener to the recognition result event
  recognition.addEventListener('result', function (event) {
    // Get the transcript of the speech input
    const transcript = event.results[0][0].transcript;
    // Display the transcript in the #waves div
    canvas.textContent = transcript;
  });
} else {
  // Display a message if the browser does not support the Web Speech API
  console.log('Speech recognition not supported.');
}
</script>
